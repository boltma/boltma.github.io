[{"title":"Fisher Information","date":"2019-05-02T14:49:24.000Z","path":"2019/05/02/Fisher-Information/","text":"Fisher InformationFisher Information（费希尔信息）是用以衡量观测数据所蕴含的数据量，具体说来是指观测所得的随机变量$X$携带的关于未知参数$\\theta$的信息量，其中$X$的概率分布依赖于$\\theta$，通常记作${\\mathcal {I}}_{X}(\\theta)$。 对于概率分布$f(X;\\theta)$（满足$f(x;\\theta )\\geq0, \\int f(x;\\theta )dx=1,\\quad\\forall\\theta\\in\\Theta$，$\\Theta$为参数集），称对数似然函数关于$\\theta$的偏导$\\frac {\\partial }{\\partial \\theta }\\log f(X;\\theta )$为其得分（Score function），对于真实的参数$\\theta$，则有此得分的期望为0： ${\\displaystyle {\\begin{aligned}\\operatorname {E} \\left[\\left.{\\frac {\\partial }{\\partial \\theta }}\\log f(X;\\theta )\\right|\\theta \\right]&amp;=\\int {\\frac { {\\frac {\\partial }{\\partial \\theta }}f(x;\\theta )}{f(x;\\theta )}}f(x;\\theta )\\,dx\\&amp;={\\frac {\\partial }{\\partial \\theta }}\\int f(x;\\theta )\\,dx\\&amp;={\\frac {\\partial }{\\partial \\theta }}1=0.\\end{aligned}}}$ 并将得分的方差定义为费希尔信息： ${\\displaystyle {\\mathcal {I}}(\\theta )=\\operatorname {E} \\left[\\left.\\left({\\frac {\\partial }{\\partial \\theta }}\\log f(X;\\theta )\\right){2}\\right|\\theta \\right]=\\int \\left({\\frac {\\partial }{\\partial \\theta }}\\log f(x;\\theta )\\right){2}f(x;\\theta )\\,dx}$ 并且在密度函数具有良好性质的情况下，可以用分布积分很容易证明： ${\\mathcal {I}}(\\theta )=-\\operatorname {E} \\left[\\left.{\\frac {\\partial {2}}{\\partial \\theta {2}}}\\log f(X;\\theta )\\right|\\theta \\right]$ 这个表达式以如下方式表达了观测所携带的信息量：若对数似然函数较为平坦，则我们对$\\theta$的估计较差，反之，若对数似然函数有高而窄的峰，则我们可以得到对$\\theta$的较好估计，而这个性状由负二阶导数表征。 由于n个样本的对数似然函数为单个似然函数之和，容易证明，n个独立同分布样本的费希尔信息是单个样本的费希尔信息的n倍。 Cramér-Rao Bound费希尔信息的重要性在于Cramér-Rao不等式，或Cramér-Rao Bound（克拉美罗界）： 费希尔信息的倒数是参数$\\theta$的任何无偏估计$\\hat\\theta$的方差的下界，即$\\operatorname {var} ({\\hat {\\theta }})\\geq {\\frac {1}{\\mathcal {I}(\\theta )}}$ 关于参数$\\theta$的估计$\\hat\\theta$的偏定义为估计的误差的期望值，即$\\operatorname {Bias} _{\\theta }[\\,{\\hat {\\theta }}\\,]=\\operatorname {E} _{\\theta }[\\,{\\hat {\\theta }}\\,]-\\theta =\\operatorname {E} _{\\theta }[\\,{\\hat {\\theta }}-\\theta \\,]$，其中$\\operatorname {E} _{\\theta }$表示期望是相对密度函数$f(X;\\theta)$而言的。 若对于所有$\\theta\\in\\Theta$，偏为0，则称此估计为无偏估计。 例如样本均值是总体均值的无偏估计量，样本方差是总体方差的无偏估计量，而标准差是总体标准差的有偏估计量。 估计无偏并不能保证误差以极大的概率是低的。例如对于正态分布${\\mathcal {N}}(\\theta ,1)$，设$X_1,X_2,\\cdots,X_n$是抽自它的独立同分布样本。估计$\\theta$时，$X_1$与$\\bar {X_n}$均为无偏估计，然而显然使用更多数据会得到更好的估计，事实上也可以证明$\\bar {X_n}$是最小方差无偏估计量，也即达到了克拉美罗界。 达到克拉美罗界的无偏估计量优越于其他所有估计，也即$\\operatorname{E}[(\\hat\\theta_1(X_1,X_2,\\cdots,X_n)-\\theta)2]\\leq\\operatorname{E}[(\\hat\\theta_2(X_1,X_2,\\cdots,X_n)-\\theta)2]$，若$\\hat\\theta_1$达到了克拉美罗界。 Cramér-Rao不等式的证明： 设$V$是得分函数，$\\hat\\theta$是估计量。由Cauchy-Schwartz不等式，可得 $$\\operatorname{E}\\theta[(V-\\operatorname{E}\\theta V)(\\hat\\theta-\\operatorname{E}\\theta\\hat\\theta)] \\leq \\operatorname{E}\\theta(V-\\operatorname{E}\\theta V)2\\operatorname{E}\\theta(\\hat\\theta-\\operatorname{E}_\\theta \\hat\\theta)2$$ 由于$\\hat\\theta$是无偏估计，所以对于任意$\\theta$均有$\\operatorname{E}\\theta \\hat\\theta=\\theta$，同时得分函数的期望也为零（见上），并且结合费希尔信息的定义（${\\mathcal {I}}(\\theta )=\\operatorname {E}[V^2]$），代入上式有$\\operatorname {E}\\theta[V\\hat\\theta]\\leq\\mathcal{I}(\\theta)\\operatorname{var}(\\hat\\theta).$ 而 $\\begin{aligned} \\operatorname {E}\\theta[V\\hat\\theta]&amp;=\\int\\frac{\\frac{\\partial}{\\partial\\theta}f(x;\\theta)}{f(x;\\theta)}\\hat\\theta(x)f(x;\\theta)dx \\ &amp;= \\int \\frac{\\partial}{\\partial\\theta} f(x;\\theta)\\hat\\theta(x)dx \\ &amp;= \\frac{\\partial}{\\partial\\theta}\\int f(x;\\theta)\\hat\\theta(x)dx \\ &amp;= \\frac{\\partial}{\\partial\\theta} \\operatorname{E}\\theta[\\hat\\theta] \\ &amp;= \\frac{\\partial}{\\partial\\theta}\\theta = 1\\end{aligned}$ （这里能交换积分与微分号是假定密度函数具有良好性质，上同） 代入即得到Cramér-Rao不等式。 $\\square$ 以相同的证明方式可以得到对于任意估计量有$\\operatorname {var} \\left({\\hat {\\theta }}\\right)\\geq {\\frac {[1+b’(\\theta )]^{2}}{\\mathcal{I}(\\theta )}}$，此处$b(\\theta)=\\operatorname{E}_\\theta[\\hat\\theta]-\\theta.$ 多参数情形的Fisher Information多参数下有费希尔信息矩阵$\\mathcal{I}(\\theta)$，其中元素为$\\mathcal{I}_{m,k}=\\operatorname {E} \\left[{\\frac {\\partial }{\\partial \\theta _{m}}}\\log f\\left(x;\\theta\\right){\\frac {\\partial }{\\partial \\theta _{k}}}\\log f\\left(x;\\theta\\right)\\right]=-\\operatorname {E} \\left[{\\frac {\\partial ^{2}}{\\partial \\theta _{m}\\partial \\theta _{k}}}\\log f\\left(x;\\theta\\right)\\right]$ Cramér-Rao不等式变为：$\\Sigma\\geq\\mathcal{I}^{-1}(\\theta)$，这里矩阵不小于号指差是半正定的，$\\Sigma$是关于$\\theta$的一组无偏估计量的协方差矩阵。","tags":[]},{"title":"形式语言与自动机","date":"2019-03-24T01:54:17.000Z","path":"2019/03/24/形式语言与自动机/","text":"上下文无关文法设计上下文无关文法 {anbncmdm|n≥1,m≥1}∩{anbmcmdn|n≥1,m≥1} S-&gt;AB|TT-&gt;aTd|aCdA-&gt;aAb|abB-&gt;cBd|cdC-&gt;bCc|bc {anbm|n,m≥0∧n≥m} S-&gt;A|BA-&gt;aA|aCB-&gt;Bb|CbC-&gt;aCb|ε {anbm|n≥0,m≥0,3n≥m≥2n} S-&gt;aSbb|aSbbb|ε {w|w∈{a,b}*,w中a和b的数目不同} S-&gt;A|BA-&gt;AA|TaB-&gt;BB|TbT-&gt;aTbT|bTaT|ε注意此处T生成a与b数目相同的字符串 {w|w∈{a,b}*,且w中a与b的数目相差为2} S-&gt;TaTaT|TbTbTT-&gt;aTbT|bTaT|ε 文法和语言中的二义性文法无二义性：语法分析树唯一，亦等价于最左推导唯一 下面的文法生成的是具有x和y的操作数、二元运算符+、-和*的前缀表达式： E-&gt;+EE|*EE|-EE|x|y 证明这个文法是无歧义的。（Hopcroft, 5.4.7(b)） 提纲：可证明其最左推导是唯一的，对字符串长度归纳，同时归纳证明生成的字符串w所有非空后缀字符串中操作数个数多于运算符个数。 正则语言设计正则语言 {xwxR|x,w∈(a+b)+},其中(a+b)+=(a+b)(a+b)*,xR为x的反向(即反转) a(a+b)(a+b)a+b(a+b)(a+b)(a+b) {w|w∈{a,b}∧∃x,y(x,y∈{a,b}∧w=xy∧|y|=3∧y=yR)} (a+b)*(aaa+aba+bab+bbb) {w∈{a,b}*|w中既不包含子串aa,也不包含子串bb} (ε+b)(ab)*(ε+a) {anbm|n,m≥0且n+m为偶数} (aa)(bb)+(aa)a(bb)b {w|w∈{a,b}*,|w|≥1,且w的后20位至少有一个a} (a+b)*a(a+b+ε)19 {w|w∈{a,b}*,|w|≥1,且当w以a结尾时,它的长度为奇数} ((a+b)(a+b))a+(a+b)b {w|w∈{a,b}*,|w|≥2,且w的前5位至少有一个子串aa} (a+b+ε)3aa(a+b)* {w|w∈{a,b}*,|w|≥2,且w的第2位至第5位至少有一个a} (a+b)(a+b+ε)3a(a+b)* {w|w∈{0,1}*,w至少含有3个1,且倒数第3位为1} (0+1)1(0+1)1(0+1)100+(0+1)1(0+1)1(01+10)+(0+1)111 有限自动机安利一个用来画自动机的app：http://madebyevan.com/fsm/ 设计DFA 长度至少为2且头两个字符不相同的0,1串构成的集合 0 1 -&gt;q0 q1 q2 q1 q4 q3 q2 q3 q4 *q3 q3 q3 q4 q4 q4 {w∈{a,b}*|w中不包含子串aa} a b -&gt;*q0 q1 q0 *q1 q2 q0 q2 q2 q2 {w∈{a,b}*|w中包含且仅包含奇数个子串ab} a b -&gt;q0 q1 q0 q1 q1 q2 *q2 q3 q2 q3 q3 q0 {w∈{a,b}*|w中a的个数和b的个数之和是奇数} a b -&gt;q0 q1 q1 *q1 q0 q0 {w∈{a,b}*|w含相同个数的a和b,且w的每个前缀中a和b个数之差不超过1} a b -&gt;*q0 q1 q2 q1 q3 q0 q2 q0 q3 q3 q3 q3 {w∈{a,b}*|w包含子串ab，但不包含子串bb} 相应的NFA有： DFA的最小化 构造与该DFA等价的最小化的DFA 填表算法第一步区分1，3，6与2，4，5 第二步区分2与4，5（输入字符b） 第三步区分1，3与6（输入字符a） 故最终等价类有{1, 3}, {2}, {4, 5}, {6} 最小化的DFA是","tags":[]},{"title":"Hello, world!","date":"2019-03-24T01:53:12.000Z","path":"2019/03/24/Hello-world/","text":"这里会记录一些学习笔记，或者灌水，请多多指教！","tags":[]}]